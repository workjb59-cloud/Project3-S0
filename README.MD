# OpenSooq Kuwait Scraper

Automated web scraper for opensooq.com (Kuwait) that extracts shop information and their advertisements, saving data to AWS S3 as Excel files.

## ğŸ¯ Features

- âœ… Scrapes all shops from opensooq.com Kuwait Ù…ØªØ§Ø¬Ø± section
- âœ… Filters and extracts **yesterday's ads only**
- âœ… Saves ad data as Excel files to AWS S3
- âœ… Maintains incremental shop info database
- âœ… Runs automatically via GitHub Actions (daily schedule)
- âœ… Uses BeautifulSoup for HTML parsing
- âœ… Respects server rate limits

## ğŸ“ Project Structure

```
Project3-S0/
â”œâ”€â”€ utils.py                    # Shared utilities (HTML parsing, S3, etc.)
â”œâ”€â”€ requirements.txt            # Python dependencies
â”œâ”€â”€ Shops/                      # Shops category scraper
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Category-specific configuration
â”‚   â”œâ”€â”€ scraper.py             # Shops scraping logic
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ .github/
â”‚   â””â”€â”€ workflows/
â”‚       â””â”€â”€ scraper.yml        # Daily automation
â””â”€â”€ README.MD
```

## ğŸ“ S3 Data Structure

```
opensooq-data/
â”œâ”€â”€ shops/                          # Shops category data
â”‚   â”œâ”€â”€ year=2026/
â”‚   â”‚   â”œâ”€â”€ month=01/
â”‚   â”‚   â”‚   â”œâ”€â”€ day=13/
â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ excel files/
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ Ø´Ø±ÙƒØ©_Ø¯Ø§Ø±_Ø§Ù„Ø´ÙˆÙŠØ®_Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠØ©.xlsx
â”‚   â”‚   â”‚   â”‚   â”‚   â”œâ”€â”€ UNLIMITED_TEC.xlsx
â”‚   â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ images/
â”‚   â”‚   â”‚   â”‚       â”œâ”€â”€ Ø´Ø±ÙƒØ©_Ø¯Ø§Ø±_Ø§Ù„Ø´ÙˆÙŠØ®_Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠØ©/
â”‚   â”‚   â”‚   â”‚       â”‚   â”œâ”€â”€ ad_275635211_main.jpg
â”‚   â”‚   â”‚   â”‚       â”‚   â””â”€â”€ ...
â”‚   â”‚   â”‚   â”‚       â””â”€â”€ UNLIMITED_TEC/
â”‚   â”‚   â”‚   â”‚           â””â”€â”€ ...
â”‚   â”‚   â”‚   â”œâ”€â”€ day=14/
â”‚   â”‚   â”‚   â”‚   â””â”€â”€ ...
â”‚   â”‚   â””â”€â”€ month=02/
â”‚   â”‚       â””â”€â”€ ...
â”œâ”€â”€ cars/                           # Future: Cars category
â”‚   â””â”€â”€ year=2026/...
â”œâ”€â”€ real-estate/                    # Future: Real estate category
â”‚   â””â”€â”€ year=2026/...
â””â”€â”€ info/
    â””â”€â”€ info.xlsx                   # Incremental shop details (all categories)
```

### Shops Folder (`opensooq-data/shops/`)
- **Format**: `year=YYYY/month=MM/day=DD/{shop_name}.xlsx` (Hive-style partitioning)
- **Content**: Yesterday's ads with basic shop info
- **Benefits**: Easy date-based querying, better organization for analytics
- **Columns**:
  - Shop: `shop_member_id`, `shop_name`
  - Ad: `ad_id`, `title`, `posted_at`, `price_amount`, `city_label`, `nhood_label`, etc.

### Info Folder (`opensooq-data/info/`)
- **File**: `info.xlsx`
- **Content**: Incremental database of all shops
- **Columns**: `member_id`, `shop_name`, `category_name`, `posts_count`, `views_count`, `average_rating`, `followers_count`, etc.
- **Updates**: Each row represents a shop; existing shops are updated with latest data

## ğŸš€ Setup

### 1. Clone Repository

```bash
git clone <your-repo-url>
cd Project3-S0
```

### 2. Install Dependencies

```bash
pip install -r requirements.txt
```

### 3. Configure AWS Credentials

#### For Local Testing:
Set environment variables:

```bash
# Windows (PowerShell)
$env:AWS_ACCESS_KEY_ID="your-access-key"
$env:AWS_SECRET_ACCESS_KEY="your-secret-key"
$env:AWS_BUCKET_NAME="your-bucket-name"

# Linux/Mac
export AWS_ACCESS_KEY_ID="your-access-key"
export AWS_SECRET_ACCESS_KEY="your-secret-key"
export AWS_BUCKET_NAME="your-bucket-name"
```

#### For GitHub Actions:
Add secrets to your GitHub repository:

1. Go to repository **Settings** â†’ **Secrets and variables** â†’ **Actions**
2. Click **New repository secret**
3. Add the following secrets:
   - `AWS_ACCESS_KEY_ID`
   - `AWS_SECRET_ACCESS_KEY`
   - `AWS_BUCKET_NAME`

### 4. Run Scraper

```bash
# Run Shops category scraper
python Shops/scraper.py

# Or as a module
python -m Shops.scraper
```

## âš™ï¸ GitHub Actions Workflow

The scraper runs automatically every day at 2:00 AM UTC.

**Workflow file**: [`.github/workflows/scraper.yml`](.github/workflows/scraper.yml)

### Schedule
```yaml
schedule:
  - cron: '0 2 * * *'  # Daily at 2:00 AM UTC
```

### Manual Trigger
You can also trigger the workflow manually from the **Actions** tab in GitHub.

## ğŸ“Š Data Fields

### Ad Data (shops/*.xlsx)

| Field | Description |
|-------|-------------|
| `shop_member_id` | Shop's unique member ID |
| `shop_name` | Name of the shop |
| `ad_id` | Advertisement ID |
| `title` | Ad title |
| `posted_at` | When ad was posted (Arabic format) |
| `expired_at` | Ad expiration date |
| `price_amount` | Price in KWD |
| `city_label` | City name (Arabic) |
| `nhood_label` | Neighborhood name (Arabic) |
| `cat1_label` | Main category |
| `cat2_label` | Sub-category |
| `highlights` | Key features |
| `masked_description` | Ad description |
| `post_url` | URL to ad detail page |
| `verification_level` | Shop verification level |
| `scraped_at` | Timestamp when scraped |

### Shop Info (info/info.xlsx)

| Field | Description |
|-------|-------------|
| `member_id` | Shop's unique member ID |
| `shop_name` | Name of the shop |
| `category_name` | Shop category |
| `posts_count` | Total number of posts |
| `views_count` | Total views |
| `response_time` | Average response time |
| `member_since` | Registration date |
| `followers_count` | Number of followers |
| `average_rating` | Average rating (out of 5) |
| `number_of_rating` | Total ratings |
| `city_name` | Shop location |
| `latitude` / `longitude` | GPS coordinates |
| `verification_level` | Verification level (0-2) |
| `scraped_at` | Last update timestamp |

## ğŸ” How It Works

1. **Fetch Shops List**: Scrapes all pages of the shops listing
2. **Parse HTML**: Uses BeautifulSoup to extract JSON data from `__NEXT_DATA__` script tag
3. **Filter Yesterday's Ads**: Checks `posted_at` field for "Ø£Ù…Ø³", "Ù‚Ø¨Ù„ ÙŠÙˆÙ…", or hours/minutes
4. **Download Images**: Downloads ad images from opensooq CDN
5. **Save to S3**:
   - Ad data â†’ `opensooq-data/shops/year=YYYY/month=MM/day=DD/excel files/{shop_name}.xlsx`
   - Ad images â†’ `opensooq-data/shops/year=YYYY/month=MM/day=DD/images/{shop_name}/ad_{id}_main.jpg`
   - Shop info â†’ `opensooq-data/info/info.xlsx` (incremental)
6. **Rate Limiting**: 2-3 second delays between requests

## ğŸ› ï¸ Key Functions

### `utils.py` (Shared utilities)

- `extract_json_from_html()` - Extract JSON from Next.js HTML
- `is_yesterday_ad()` - Check if ad was posted yesterday
- `filter_yesterday_ads()` - Filter ads list
- `get_partitioned_s3_path()` - Generate Hive-style partitioned S3 paths
- `download_image()` - Download image from URL
- `upload_image_to_s3()` - Upload image bytes to S3
- `save_ad_images_to_s3()` - Download and save all ad images
- `upload_to_s3()` - Upload DataFrame to S3 as Excel
- `download_from_s3()` - Download Excel from S3
- `update_incremental_info()` - Update shop info incrementally

### `Shops/scraper.py` (Category-specific)

- `get_shops_list()` - Fetch shops from listing page
- `get_shop_details_and_ads()` - Fetch individual shop data
- `process_shop()` - Process shop and save to S3
- `main()` - Orchestrate entire scraping process

### `Shops/config.py` (Category configuration)

- Category URLs and names
- S3 paths
- Scraping delays and settings
- Field definition
- `get_shops_list()` - Fetch shops from listing page
- `get_shop_details_and_ads()` - Fetch individual shop data
- `process_shop()` - Process shop and save to S3
- `main()` - Orchestrate entire scraping process

## ğŸ“ Example Output

```
============================================================
OpenSooq Scraper - Starting
Date: 2026-01-13 14:30:00
============================================================
Fetching shops list...
============================================================

Fetching shops page 1...
Found 30 shops on page 1
Progress: Page 1 of 3

Fetching shops page 2...
Found 30 shops on page 2
Progress: Page 2 of 3

Fetching shops page 3...
Found 8 shops on page 3
Progress: Page 3 of 3

Reached last page. Total shops: 68

Total shops to process: 68

============================================================
Processing shops and their ads...
============================================================

[1/68] Processing shop: Ø´Ø±ÙƒØ© Ø¯Ø§Ø± Ø§Ù„Ø´ÙˆÙŠØ® Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠØ© (ID: 75063309)
URL: Ø´Ø±ÙƒØ©-Ø¯Ø§Ø±-Ø§Ù„Ø´ÙˆÙŠØ®-Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠØ©-75063309
Total ads: 25, Yesterday's ads: 3
Successfully uploaded to s3://your-bucket/opensooq-data/shops/year=2026/month=01/day=13/Ø´Ø±ÙƒØ©_Ø¯Ø§Ø±_Ø§Ù„Ø´ÙˆÙŠØ®_Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠØ©.xlsx
âœ“ Successfully processed shop: Ø´Ø±ÙƒØ© Ø¯Ø§Ø± Ø§Ù„Ø´ÙˆÙŠØ® Ø§Ù„Ø¹Ù‚Ø§Ø±ÙŠØ©

...

============================================================
Scraping Complete!
============================================================
Total shops: 68
Successfully processed: 65
Errors: 3
Completion time: 2026-01-13 14:45:30
============================================================
```

## ğŸ”’ Security Notes

- Never commit AWS credentials to the repository
- Use GitHub Secrets for sensitive data
- EğŸ”„ Adding New Categories

The project is structured to easily support multiple categories:

1. **Create category folder** (e.g., `Cars/`, `RealEstate/`)
2. **Copy structure from `Shops/`**:
   ```
   Cars/
   â”œâ”€â”€ __init__.py
   â”œâ”€â”€ config.py        # Update URLs, category name, S3 path
   â”œâ”€â”€ scraper.py       # Customize if needed (or reuse)
   â””â”€â”€ README.md
   ```
3. **Update `config.py`** with category-specific settings
4. **Add to GitHub Actions** workflow
5. **Shared utilities** in root `utils.py` work for all categories

### Example: Adding Cars Category

```python
# Cars/config.py
CATEGORY = "cars"
CATEGORY_NAME_AR = "Ø³ÙŠØ§Ø±Ø§Øª"
CATEGORY_URL = "https://kw.opensooq.com/ar/cars"
S3_CATEGORY_PATH = "cars"
```

Then update workflow:
```yaml
- name: Run Cars scraper
  run: python Cars/scraper.py
```

## nsure S3 bucket has proper access policies
- Consider using IAM roles instead of access keys for production

## ğŸ“„ License

This project is for educational purposes. Please respect opensooq.com's terms of service and robots.txt.

## ğŸ¤ Contributing

Feel free to open issues or submit pull requests for improvements.

---

**Last Updated**: January 13, 2026

