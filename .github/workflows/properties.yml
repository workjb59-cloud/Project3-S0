name: Properties

on:
  schedule:
    # Run daily at 12:00 AM UTC (adjust as needed)
    - cron: '0 0 * * *'
  
  # Allow manual trigger from Actions tab
  workflow_dispatch:
    inputs:
      categories:
        description: 'Categories to scrape'
        required: false
        default: 'rental sale'
        type: string

jobs:
  scrape-properties:
    name: Scrape Properties
    runs-on: ubuntu-latest
    timeout-minutes: 60

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Set up Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'
          cache: 'pip'

      - name: Install dependencies
        run: |
          python -m pip install --upgrade pip
          pip install -r requirements.txt

      - name: Run Properties Scraper
        env:
          AWS_S3_BUCKET: ${{ secrets.AWS_S3_BUCKET }}
          AWS_ACCESS_KEY: ${{ secrets.AWS_ACCESS_KEY_ID }}
          AWS_SECRET_KEY: ${{ secrets.AWS_SECRET_ACCESS_KEY }}
        run: |
          cd Properties
          python -m __main__ --categories ${{ github.event.inputs.categories || 'rental sale' }}

      - name: Upload logs on failure
        if: failure()
        uses: actions/upload-artifact@v4
        with:
          name: scraper-logs
          path: logs/
          retention-days: 7

      - name: Notify on success
        if: success()
        run: |
          echo "✓ Properties scraping completed successfully"
          echo "Data uploaded to S3 bucket: ${{ secrets.AWS_S3_BUCKET }}"

      - name: Notify on failure
        if: failure()
        run: |
          echo "✗ Properties scraping failed"
          echo "Check the logs for details"
          exit 1
